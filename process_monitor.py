#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ç”Ÿå›¾æµç¨‹ç›‘æ§å™¨ - åå°è¿›ç¨‹ç›‘æ§å’Œæ‰§è¡Œæµç¨‹æ•è·
ç›‘æ§ComfyUIåå°æ‰§è¡Œè¿‡ç¨‹ï¼Œæ•è·å®Œæ•´çš„ç”Ÿå›¾æµç¨‹ï¼Œå¹¶æ”¯æŒæ¨¡æ‹ŸéªŒè¯
"""

import os
import sys
import json
import time
import requests
import logging
import threading
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import subprocess
import traceback

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


class ExecutionFlowCapture:
    """æ‰§è¡Œæµç¨‹æ•è·å™¨"""
    
    def __init__(self, comfyui_port=8187):
        self.comfyui_url = f"http://127.0.0.1:{comfyui_port}"
        self.capture_dir = "captured_flows"
        os.makedirs(self.capture_dir, exist_ok=True)
        
        # æ•è·çš„æ•°æ®
        self.api_calls = []  # APIè°ƒç”¨è®°å½•
        self.execution_steps = []  # æ‰§è¡Œæ­¥éª¤è®°å½•
        self.workflow_data = None  # å·¥ä½œæµæ•°æ®
        self.monitoring = False
        self.monitor_thread = None
        
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        self.api_calls = []
        self.execution_steps = []
        
        logger.info("=" * 60)
        logger.info("ğŸ” å¼€å§‹ç›‘æ§åå°æ‰§è¡Œæµç¨‹")
        logger.info("=" * 60)
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2)
        
        logger.info("â¹ï¸ åœæ­¢ç›‘æ§")
        
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        last_queue_check = 0
        last_history_check = {}
        
        while self.monitoring:
            try:
                current_time = time.time()
                
                # 1. ç›‘æ§é˜Ÿåˆ—çŠ¶æ€
                if current_time - last_queue_check > 1:
                    self._check_queue()
                    last_queue_check = current_time
                
                # 2. ç›‘æ§å†å²è®°å½•ï¼ˆæ•è·æ‰§è¡Œç»†èŠ‚ï¼‰
                self._check_history(last_history_check)
                
                time.sleep(0.5)
                
            except Exception as e:
                logger.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                
    def _check_queue(self):
        """æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€"""
        try:
            response = requests.get(f"{self.comfyui_url}/queue", timeout=2)
            if response.status_code == 200:
                queue_data = response.json()
                
                # è®°å½•é˜Ÿåˆ—å˜åŒ–
                running = queue_data.get('queue_running', [])
                pending = queue_data.get('queue_pending', [])
                
                if running:
                    for item in running:
                        prompt_id = item[1]
                        self._log_execution_step(
                            "QUEUE_RUNNING",
                            {"prompt_id": prompt_id, "status": "executing"}
                        )
                        
        except requests.exceptions.RequestException:
            pass  # å¿½ç•¥ç½‘ç»œé”™è¯¯
            
    def _check_history(self, last_check: dict):
        """æ£€æŸ¥æ‰§è¡Œå†å²"""
        try:
            response = requests.get(f"{self.comfyui_url}/history", timeout=2)
            if response.status_code == 200:
                history_data = response.json()
                
                # æ£€æŸ¥æ–°çš„æ‰§è¡Œè®°å½•
                for prompt_id, data in history_data.items():
                    if prompt_id not in last_check:
                        self._process_execution_history(prompt_id, data)
                        last_check[prompt_id] = data
                        
        except requests.exceptions.RequestException:
            pass
            
    def _process_execution_history(self, prompt_id: str, history_data: dict):
        """å¤„ç†æ‰§è¡Œå†å²"""
        logger.info(f"ğŸ“Š æ•è·åˆ°æ‰§è¡Œè®°å½•: {prompt_id[:8]}...")
        
        # æå–å·¥ä½œæµ
        prompt = history_data.get('prompt', [])
        if len(prompt) >= 3:
            workflow = prompt[2]
            self.workflow_data = workflow
            
            self._log_execution_step(
                "WORKFLOW_CAPTURED",
                {
                    "prompt_id": prompt_id,
                    "workflow": workflow,
                    "node_count": len(workflow) if isinstance(workflow, dict) else 0
                }
            )
            
        # æå–è¾“å‡ºä¿¡æ¯
        outputs = history_data.get('outputs', {})
        if outputs:
            self._log_execution_step(
                "OUTPUTS_GENERATED",
                {
                    "prompt_id": prompt_id,
                    "outputs": outputs
                }
            )
            
    def _log_execution_step(self, step_type: str, data: dict):
        """è®°å½•æ‰§è¡Œæ­¥éª¤"""
        step = {
            "timestamp": datetime.now().isoformat(),
            "type": step_type,
            "data": data
        }
        self.execution_steps.append(step)
        
        # æ§åˆ¶å°è¾“å‡º
        if step_type == "WORKFLOW_CAPTURED":
            logger.info(f"  âœ“ æ•è·å·¥ä½œæµ (èŠ‚ç‚¹æ•°: {data.get('node_count', 0)})")
        elif step_type == "OUTPUTS_GENERATED":
            logger.info(f"  âœ“ ç”Ÿæˆå®Œæˆ")
            
    def capture_api_call(self, method: str, endpoint: str, data: Any = None, response: Any = None):
        """æ•è·APIè°ƒç”¨"""
        call = {
            "timestamp": datetime.now().isoformat(),
            "method": method,
            "endpoint": endpoint,
            "request_data": data,
            "response_data": response
        }
        self.api_calls.append(call)
        
        logger.info(f"ğŸ“¡ APIè°ƒç”¨: {method} {endpoint}")
        
    def save_capture(self) -> str:
        """ä¿å­˜æ•è·çš„æµç¨‹"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"flow_capture_{timestamp}.json"
        filepath = os.path.join(self.capture_dir, filename)
        
        capture_data = {
            "captured_at": datetime.now().isoformat(),
            "workflow": self.workflow_data,
            "execution_steps": self.execution_steps,
            "api_calls": self.api_calls,
            "summary": {
                "total_steps": len(self.execution_steps),
                "total_api_calls": len(self.api_calls),
                "has_workflow": self.workflow_data is not None
            }
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(capture_data, f, indent=2, ensure_ascii=False)
            
        logger.info(f"ğŸ’¾ æµç¨‹å·²ä¿å­˜: {filepath}")
        return filepath


class FlowSimulator:
    """æµç¨‹æ¨¡æ‹Ÿå™¨ - é‡æ”¾æ•è·çš„æµç¨‹è¿›è¡ŒéªŒè¯"""
    
    def __init__(self, comfyui_port=8187):
        self.comfyui_url = f"http://127.0.0.1:{comfyui_port}"
        
    def load_captured_flow(self, filepath: str) -> dict:
        """åŠ è½½æ•è·çš„æµç¨‹"""
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
            
    def simulate_flow(self, flow_data: dict, dry_run: bool = False) -> bool:
        """æ¨¡æ‹Ÿæ‰§è¡Œæµç¨‹
        
        Args:
            flow_data: æ•è·çš„æµç¨‹æ•°æ®
            dry_run: å¦‚æœä¸ºTrueï¼ŒåªéªŒè¯ä¸å®é™…æ‰§è¡Œ
            
        Returns:
            æ˜¯å¦éªŒè¯æˆåŠŸ
        """
        logger.info("=" * 60)
        logger.info("ğŸ”„ å¼€å§‹æ¨¡æ‹Ÿæ‰§è¡Œæµç¨‹")
        logger.info("=" * 60)
        
        workflow = flow_data.get('workflow')
        if not workflow:
            logger.error("âŒ æµç¨‹æ•°æ®ä¸­æ²¡æœ‰å·¥ä½œæµ")
            return False
            
        logger.info(f"ğŸ“‹ å·¥ä½œæµèŠ‚ç‚¹æ•°: {len(workflow)}")
        
        # 1. éªŒè¯å·¥ä½œæµç»“æ„
        if not self._validate_workflow(workflow):
            logger.error("âŒ å·¥ä½œæµç»“æ„éªŒè¯å¤±è´¥")
            return False
            
        logger.info("âœ“ å·¥ä½œæµç»“æ„éªŒè¯é€šè¿‡")
        
        # 2. åˆ†æèŠ‚ç‚¹ä¾èµ–
        node_graph = self._analyze_node_dependencies(workflow)
        logger.info(f"âœ“ èŠ‚ç‚¹ä¾èµ–åˆ†æå®Œæˆ (å…±{len(node_graph)}ä¸ªèŠ‚ç‚¹)")
        
        # 3. å¦‚æœä¸æ˜¯dry_runï¼Œå®é™…æ‰§è¡Œ
        if not dry_run:
            success = self._execute_workflow(workflow)
            if success:
                logger.info("âœ… æµç¨‹æ‰§è¡ŒæˆåŠŸ")
            else:
                logger.error("âŒ æµç¨‹æ‰§è¡Œå¤±è´¥")
            return success
        else:
            logger.info("âœ“ éªŒè¯æ¨¡å¼ - è·³è¿‡å®é™…æ‰§è¡Œ")
            return True
            
    def _validate_workflow(self, workflow: dict) -> bool:
        """éªŒè¯å·¥ä½œæµç»“æ„"""
        if not isinstance(workflow, dict):
            return False
            
        for node_id, node_data in workflow.items():
            if not isinstance(node_data, dict):
                logger.error(f"èŠ‚ç‚¹ {node_id} æ ¼å¼é”™è¯¯")
                return False
                
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            if 'class_type' not in node_data:
                logger.error(f"èŠ‚ç‚¹ {node_id} ç¼ºå°‘ class_type")
                return False
                
        return True
        
    def _analyze_node_dependencies(self, workflow: dict) -> dict:
        """åˆ†æèŠ‚ç‚¹ä¾èµ–å…³ç³»"""
        node_graph = {}
        
        for node_id, node_data in workflow.items():
            dependencies = []
            
            # åˆ†æè¾“å…¥ä¸­çš„ä¾èµ–
            inputs = node_data.get('inputs', {})
            for input_name, input_value in inputs.items():
                if isinstance(input_value, list) and len(input_value) >= 1:
                    # [node_id, output_index] æ ¼å¼
                    if isinstance(input_value[0], str):
                        dependencies.append(input_value[0])
                        
            node_graph[node_id] = {
                "class_type": node_data.get('class_type'),
                "dependencies": dependencies
            }
            
            logger.info(f"  èŠ‚ç‚¹ {node_id}: {node_data.get('class_type')} (ä¾èµ–: {len(dependencies)}ä¸ª)")
            
        return node_graph
        
    def _execute_workflow(self, workflow: dict) -> bool:
        """æ‰§è¡Œå·¥ä½œæµ"""
        try:
            logger.info("ğŸ“¤ æäº¤å·¥ä½œæµåˆ°ComfyUI...")
            
            response = requests.post(
                f"{self.comfyui_url}/prompt",
                json={"prompt": workflow},
                timeout=10
            )
            
            if response.status_code != 200:
                logger.error(f"æäº¤å¤±è´¥: HTTP {response.status_code}")
                return False
                
            result = response.json()
            prompt_id = result.get('prompt_id')
            
            if not prompt_id:
                logger.error("æœªè·å–åˆ°prompt_id")
                return False
                
            logger.info(f"âœ“ æäº¤æˆåŠŸ (ID: {prompt_id[:8]}...)")
            
            # ç­‰å¾…æ‰§è¡Œå®Œæˆ
            return self._wait_for_completion(prompt_id)
            
        except Exception as e:
            logger.error(f"æ‰§è¡Œé”™è¯¯: {e}")
            traceback.print_exc()
            return False
            
    def _wait_for_completion(self, prompt_id: str, timeout: int = 300) -> bool:
        """ç­‰å¾…æ‰§è¡Œå®Œæˆ"""
        start_time = time.time()
        last_log_time = start_time
        
        logger.info("â³ ç­‰å¾…æ‰§è¡Œå®Œæˆ...")
        
        while time.time() - start_time < timeout:
            try:
                response = requests.get(f"{self.comfyui_url}/history/{prompt_id}", timeout=5)
                if response.status_code == 200:
                    history = response.json()
                    if prompt_id in history:
                        logger.info("âœ“ æ‰§è¡Œå®Œæˆ")
                        
                        # æ˜¾ç¤ºè¾“å‡ºä¿¡æ¯
                        outputs = history[prompt_id].get('outputs', {})
                        if outputs:
                            logger.info(f"âœ“ ç”Ÿæˆäº† {len(outputs)} ä¸ªè¾“å‡º")
                        return True
                        
                # æ¯5ç§’è¾“å‡ºä¸€æ¬¡ç­‰å¾…ä¿¡æ¯
                current_time = time.time()
                if current_time - last_log_time >= 5:
                    elapsed = int(current_time - start_time)
                    logger.info(f"  ç­‰å¾…ä¸­... ({elapsed}ç§’)")
                    last_log_time = current_time
                    
            except:
                pass
                
            time.sleep(2)
            
        logger.error("âŒ æ‰§è¡Œè¶…æ—¶")
        return False


class ProcessMonitor:
    """ä¸»ç›‘æ§å™¨ - æ•´åˆæ‰€æœ‰åŠŸèƒ½"""
    
    def __init__(self, comfyui_port=8187):
        self.port = comfyui_port
        self.capture = ExecutionFlowCapture(comfyui_port)
        self.simulator = FlowSimulator(comfyui_port)
        
    def monitor_and_capture(self, duration: int = 60):
        """ç›‘æ§å¹¶æ•è·æµç¨‹
        
        Args:
            duration: ç›‘æ§æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œ0è¡¨ç¤ºæ‰‹åŠ¨åœæ­¢
        """
        logger.info("=" * 60)
        logger.info("ğŸ¯ åå°è¿›ç¨‹ç›‘æ§ç³»ç»Ÿ")
        logger.info("=" * 60)
        logger.info(f"ComfyUIç«¯å£: {self.port}")
        logger.info(f"ç›‘æ§æ—¶é•¿: {'æ‰‹åŠ¨åœæ­¢' if duration == 0 else f'{duration}ç§’'}")
        logger.info("")
        
        # å¼€å§‹ç›‘æ§
        self.capture.start_monitoring()
        
        try:
            if duration == 0:
                logger.info("æç¤º: è¯·åœ¨åŸç•Œé¢å®Œæˆç”Ÿå›¾æ“ä½œï¼Œç„¶åæŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
                while True:
                    time.sleep(1)
            else:
                logger.info(f"æç¤º: å°†ç›‘æ§ {duration} ç§’ï¼Œè¯·åœ¨æ­¤æœŸé—´å®Œæˆç”Ÿå›¾æ“ä½œ")
                for i in range(duration):
                    time.sleep(1)
                    if (i + 1) % 10 == 0:
                        logger.info(f"  å·²ç›‘æ§ {i+1}/{duration} ç§’...")
                        
        except KeyboardInterrupt:
            logger.info("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ç›‘æ§")
            
        finally:
            # åœæ­¢ç›‘æ§
            self.capture.stop_monitoring()
            
            # ä¿å­˜æ•è·ç»“æœ
            if self.capture.execution_steps or self.capture.workflow_data:
                filepath = self.capture.save_capture()
                
                logger.info("")
                logger.info("=" * 60)
                logger.info("ğŸ“Š æ•è·æ‘˜è¦")
                logger.info("=" * 60)
                logger.info(f"æ‰§è¡Œæ­¥éª¤: {len(self.capture.execution_steps)} ä¸ª")
                logger.info(f"APIè°ƒç”¨: {len(self.capture.api_calls)} æ¬¡")
                logger.info(f"å·¥ä½œæµ: {'å·²æ•è·' if self.capture.workflow_data else 'æœªæ•è·'}")
                logger.info(f"ä¿å­˜ä½ç½®: {filepath}")
                
                return filepath
            else:
                logger.warning("âš ï¸ æœªæ•è·åˆ°ä»»ä½•æ‰§è¡Œæµç¨‹")
                return None
                
    def verify_captured_flow(self, filepath: str, dry_run: bool = True):
        """éªŒè¯æ•è·çš„æµç¨‹
        
        Args:
            filepath: æ•è·æ–‡ä»¶è·¯å¾„
            dry_run: æ˜¯å¦åªéªŒè¯ä¸æ‰§è¡Œ
        """
        logger.info("")
        logger.info("=" * 60)
        logger.info("ğŸ” éªŒè¯æ•è·çš„æµç¨‹")
        logger.info("=" * 60)
        
        # åŠ è½½æµç¨‹
        flow_data = self.simulator.load_captured_flow(filepath)
        
        logger.info(f"æ–‡ä»¶: {filepath}")
        logger.info(f"æ•è·æ—¶é—´: {flow_data.get('captured_at')}")
        logger.info(f"æ‰§è¡Œæ­¥éª¤: {flow_data.get('summary', {}).get('total_steps')} ä¸ª")
        logger.info("")
        
        # æ¨¡æ‹Ÿæ‰§è¡Œ
        success = self.simulator.simulate_flow(flow_data, dry_run=dry_run)
        
        logger.info("")
        if success:
            logger.info("âœ… éªŒè¯é€šè¿‡ - æµç¨‹å¯ä»¥æ­£ç¡®é‡æ”¾")
        else:
            logger.error("âŒ éªŒè¯å¤±è´¥ - æµç¨‹å­˜åœ¨é—®é¢˜")
            
        return success


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç”Ÿå›¾æµç¨‹ç›‘æ§å’ŒéªŒè¯å·¥å…·')
    parser.add_argument('--port', type=int, default=8187, help='ComfyUIç«¯å£å·')
    parser.add_argument('--duration', type=int, default=0, help='ç›‘æ§æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œ0è¡¨ç¤ºæ‰‹åŠ¨åœæ­¢')
    parser.add_argument('--verify', type=str, help='éªŒè¯æŒ‡å®šçš„æ•è·æ–‡ä»¶')
    parser.add_argument('--execute', action='store_true', help='å®é™…æ‰§è¡Œï¼ˆè€Œä¸æ˜¯åªéªŒè¯ï¼‰')
    
    args = parser.parse_args()
    
    monitor = ProcessMonitor(args.port)
    
    if args.verify:
        # éªŒè¯æ¨¡å¼
        monitor.verify_captured_flow(args.verify, dry_run=not args.execute)
    else:
        # ç›‘æ§æ¨¡å¼
        captured_file = monitor.monitor_and_capture(args.duration)
        
        # å¦‚æœæ•è·æˆåŠŸï¼Œè‡ªåŠ¨éªŒè¯
        if captured_file:
            logger.info("")
            input("æŒ‰å›è½¦é”®å¼€å§‹éªŒè¯æ•è·çš„æµç¨‹...")
            monitor.verify_captured_flow(captured_file, dry_run=True)


if __name__ == "__main__":
    main()
